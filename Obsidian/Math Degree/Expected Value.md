---
tags:
  - Stats
---
(From [[Discrete Probability Distributions]], [[Continuous Probability Distributions]])
The expected value, or mean (also notated as $E(x)$ or $\mu_x$), is the value we will come to expect after running a probabilistic experiment a significant number of times. 

In general, the formula for expected value is as follows, however it is important to note that each probability distribution will change this formula, sometimes simplifying it.
$$\mu_{x}= E(x)= \sum\limits xp(x)$$
Where $x$ is the outcome of the experiment and $p(x)$ is the probability of that outcome.

## Additional Rules
For some constant $c$, and some function $g:\mathbb{R}\to\mathbb{R}$,
$$E[c]=c$$
$$E[cg(x)]=cE[g(x)]$$