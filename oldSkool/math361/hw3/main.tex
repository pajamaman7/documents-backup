\documentclass{article}
\input{preamble.tex}
\pagestyle{fancy}
\lhead{Assignment \# $3$}
\rhead{Name: Thomas Boyko; UCID: 30191728}
\chead{}

\usepackage{coffeestains}
\begin{document}
\begin{enumerate} 

\item An $m \times n$ matrix is said to be a queen if the restriction of A to the orthogonal complement of its kernel is an isometry.
\begin{enumerate}[label= (\alph*)] 
\item Show that $A$ is a queen if and only if $A^* A$ is an orthogonal projection.
    \paragraph{Solution: }Suppose $A$ is a queen. Then $A$ is an isometry on 

    $(\ker A)^{\perp}=(\ker A^* A)^{\perp}=\text{ran} A^* A$. Take any $v\in \mathbb{C}^{n}$, which can be decomposed as $v=x+y$ with $x\in (\ker A^* A)^\perp$ and $y\in \ker A^* A$. Then:
    \begin{align*}
        (A^* A)^2v&= (A^* A)^2(x+y) \\
            &= (A^* A)^2x +(A^* A)^2y\\
            &= (A^* A)^2x
    .\end{align*}
    But since $A$ is an isometry on $(\ker A^* A)^{\perp}$, which contains $x,$ we must have $A^* Ax=x$. Then
    \begin{align*}
        (A^* A)^2v &= (A^* A)^2x \\
        &= A^* Ax \\
        &= A^* Ax +0\\
        &= A^* Ax +A^* Ay\\
        &= A^* A(x+y)\\
        &= A^* Av
    .\end{align*}
    And therefore $(A^* A)^2=A^* A$, and $A^* A$ is an orthogonal projection.

    Conversely, let $A^* A$ be an orthogonal projection, and $v\in (\ker A)^{\perp}=\text{ran}(A^* A)$. But we know that $A^* A$ acts as identity on its range. So $A^* Av=v$, and
    \begin{align*}
       \langle A^* Av,v \rangle&= \langle v,v \rangle \\
       \langle Av,Av \rangle&= \langle v,v \rangle \\
       \|Av\|^2&= \|v\|^2 \\
       \|Av\|&= \|v\|
    .\end{align*}
    And so $A$ is an isometry on the orthogonal complement of its kernel, and $A$ is a queen.

\item Show that $A$ is a queen if and only if $AA^*$ is an orthogonal projection.
    \paragraph{Solution: }
    We already have that $A$ is a queen $\iff $ $A^* A$ is an orthogonal projection. Rather than repeat the previous argument with $A A^* $, we show that $A^* A$ is an orthogonal projection $\iff  A A^* $ is an orthogonal projection.

    $\implies:$ Let $A^* A$ be an orthogonal projection. Then take $v\in \mathbb{C}^{n}$, and since $\text{ran}A^* A=(\ker A^* A)^{\perp}=(\ker A)^{\perp}=\text{ran}A^*$, we know that $A^* v\in \text{ran}A^* A$. Then since $A^* A$ acts as identity on its range, we have:
    \[ (A A^* )^2v=A(A^* A) (A^* v)=A A^* v .\] 
    And so $A A^* $ is an orthogonal projection.

    $\impliedby:$ Let $A A^* $ be an orthogonal projection. Then take $w\in \mathbb{C}^{m}$, and since
    $\text{ran}A A^* =(\ker A A^* )^{\perp}= (\ker A^* )^{\perp}=\text{ran}A$, we know that $Aw\in \text{ran}A^* A$. Then since $A A^* $ acts as identity on its range,
    \[ (A^* A)^2w=A^* (A A^* )(Aw)=A^* Aw .\] 
    And therefore $A^* A$ is an orthogonal projection.

    Now we have: $A$ is a queen $\iff A^* A$ is an orthogonal projection $\iff$  $AA ^*$ is an orthogonal projection.

\newpage
\item Show that a queen A is an isometry if and only if $\ker A = {0}$.

    \paragraph{Solution: }If $\ker A=\{0\} $, then $(\ker A)^{\perp}=V$, so the restriction of $A$ to the orthogonal complement of its kernel is $A$ restricted to all of $V$. Then $A$ is an isometry on any vector.

    Conversely, suppose $A$ is an isometry. Then:
    \begin{align*}
        v\in \ker A&\iff Av=0\\
                   &\iff \|Av\|=\|v\|=0\\
                   &\iff v=0
    .\end{align*}
    Therefore $\ker A=\{0\} $. 

\item Find an example of a $4 \times  2$ queen that has non-zero kernel. Be sure to prove it's a queen!
    \paragraph{Solution: }Take the matrix:
    \[
        A=\begin{bmatrix} 0&1\\0&0\\ 0&0\\0&0  \end{bmatrix} 
    .\] 
    Then clearly $A\begin{bmatrix} t\\0 \end{bmatrix} =0$ for any $t\in \mathbb{C}$, so $A$ has nonzero kernel, and all we must show is that $A$ is a queen.

    Begin by observing that since $\ker A=\text{span} \left\{ \begin{bmatrix} 1\\0 \end{bmatrix}  \right\} $
    , we can find:

    $\left( \ker A \right) ^{\perp}=\text{span}\left\{ \begin{bmatrix} 0\\1 \end{bmatrix}  \right\}  $. Then let $v=\begin{bmatrix} 0\\t \end{bmatrix} \in (\ker A)^{\perp}$. Computing both $\|v\|,\|Av\|,$ we see:
    \[
    \|Av\|=\Vert \begin{bmatrix} 0&1\\0&0\\ 0&0\\0&0  \end{bmatrix} \begin{bmatrix} 0\\t \end{bmatrix} \Vert=\|\begin{bmatrix} t\\0\\0\\0  \end{bmatrix} \|=|t|=\|\begin{bmatrix} 0\\t \end{bmatrix} \|=\|v\|
    .\] 
    So the restriction of $A$ to the orthogonal complement of its kernel is an isometry, and $A$ is a queen.
\end{enumerate}
\item \begin{enumerate}[label= (\alph*)] 
\item Given a singular value decomposition $A = W \Sigma V^* $ of a square matrix $A$, construct a polar decomposition of $A$ using $W,V,\Sigma$.
    \paragraph{Solution: }Suppose $A=W\Sigma V ^* $ is given, we wish to find $|A|$ and some $U$ unitary with $A = U|A|$. 
    \[
    |A|=\sqrt{A^* A} =\sqrt{V\Sigma^* W^* W\Sigma V^* } =\sqrt{V\Sigma^* \Sigma V^* } 
    .\] 
    But recalling that $\Sigma$ is a real diagonal matrix, we have $\Sigma=\Sigma^* $:
    \[
    |A|=\sqrt{V\Sigma V^* V\Sigma V^* } =V\Sigma V^* 
    .\] 
    Now we wish to right cancel $V$, and get back our $W$. So take $U=WV^* $ as the unitary (since it is the product of unitaries); and then:
     \[
    U|A|=(WV^* )(V\Sigma V^* )=W\Sigma V^* =A
    .\] 

    \newpage
\item Using the method above, compute a polar decomposition for
\[
    A=\begin{bmatrix} 1&3\\2&6 \end{bmatrix} 
.\] 
\paragraph{Solution: }Compute $A^* A$;

\[
    A^* A=\begin{bmatrix} 1&2\\3&6 \end{bmatrix} \begin{bmatrix} 1&3\\2&6 \end{bmatrix} 
    =\begin{bmatrix} 5&15\\15&45 \end{bmatrix} 
.\] 
And find the characteristic polynomial:
\[
    C_{A^* A}(z)=\det(A-zI)=\begin{vmatrix} 5-z&15\\15&45-z \end{vmatrix}= z^2-50=z(z-50)
.\] 
    Which gives the nonzero singular value $\sigma_1=5\sqrt{2} $, and our $\Sigma=\begin{bmatrix} 5\sqrt{2} &0\\0&0 \end{bmatrix} $. Then find an associated eigenvector for $\sigma_1^2$.
    \begin{align*}
        (50I-A^* A)v_1=0&\implies \begin{bmatrix} -45&15\\15&-5 \end{bmatrix}v_1=0 \\
                        &\implies \begin{bmatrix} -3&1\\0&0\end{bmatrix}v_1=0 \\
                        &\implies v_1=t\begin{bmatrix} 1\\-3 \end{bmatrix} \\
                        &\implies v_1=\frac{1}{\sqrt{10} }\begin{bmatrix} 1\\-3 \end{bmatrix} 
    .\end{align*}
    Now that we have $v_1,$ we need only pick $v_2$ so that $V$ is unitary, so by inspection take $v_2=\frac{1}{\sqrt{10} }\begin{bmatrix} 3\\-1 \end{bmatrix} $, which already has norm $1$, and is orthogonal to $v_1$. And so we have our matrix $V=\frac{1}{\sqrt{10} }\begin{bmatrix} 1&3\\3&-1 \end{bmatrix} $.

    Now we find $W$. Begin by computing: 
    \[
        w_1=\frac{1}{\sigma_1}Av_1=\frac{1}{5\sqrt{2} }\frac{1}{\sqrt{10} }\begin{bmatrix} 1&3\\2&6 \end{bmatrix} \begin{bmatrix} 1\\3 \end{bmatrix} =\frac{1}{\sqrt{5} }\begin{bmatrix} 1\\2 \end{bmatrix} 
    .\] 
        And again by inspection, $w_2=\frac{1}{\sqrt{5} }\begin{bmatrix} -2\\1 \end{bmatrix} $, and $W^* =\frac{1}{\sqrt{5} }\begin{bmatrix} 1&2\\-2&1 \end{bmatrix} $. So then we have our SVD:
    \[
        A=\frac{1}{\sqrt{10} }\begin{bmatrix} 1&3\\3&-1 \end{bmatrix} \begin{bmatrix} 5\sqrt{2} &0\\0&0 \end{bmatrix} \frac{1}{\sqrt{5} }\begin{bmatrix} 1&2\\-2&1 \end{bmatrix} 
    .\] 
    After a quick sanity check that all our matrix multiplication gives us back $A$, we just need to find $|A|=V\Sigma V^* $ and $U=WV^* $.
    \begin{align*}
        |A|&=V\Sigma V^* \\
           &=\frac{1}{10}\begin{bmatrix} 1&3\\3&-1 \end{bmatrix} \begin{bmatrix} 5\sqrt{2} &0\\0&0 \end{bmatrix} \begin{bmatrix} 1&3\\3&-1 \end{bmatrix} \\
           &=\frac{1}{10}\begin{bmatrix} 1&3\\3&-1 \end{bmatrix} \begin{bmatrix} 5\sqrt{2} &15\sqrt{2} \\0&0 \end{bmatrix} \\
           &= \frac{1}{10}\begin{bmatrix} 5\sqrt{2} &15\sqrt{2}\\ 15\sqrt{2} &45\sqrt{2} \end{bmatrix}  \\
           &= \frac{1}{\sqrt{2} }\begin{bmatrix} 1&3\\ 3&9\end{bmatrix}  \\
        U&=WV^* \\
         &=\frac{1}{\sqrt{200} }\begin{bmatrix} 2&-4 \\4 &2 \end{bmatrix} \begin{bmatrix} 1&3\\3&-1 \end{bmatrix} \\
         &= \frac{1}{10\sqrt{2} } \begin{bmatrix} -10& 10\\10&10 \end{bmatrix} \\
         &= \frac{1}{\sqrt{2} } \begin{bmatrix} -1& 1\\1&1 \end{bmatrix}
    .\end{align*}
    And so we have the polar decomposition:
    \[
        A=\frac{1}{\sqrt{2} } \begin{bmatrix} -1& 1\\1&1 \end{bmatrix}\frac{1}{\sqrt{2} }\begin{bmatrix} 1&3\\ 3&9\end{bmatrix} =\begin{bmatrix} 1&3\\2&6 \end{bmatrix}  \\
    .\] 
\end{enumerate}

\item Find your favorite $4 \times  2$ matrix A of rank 2 and compute a singular value decomposition for A. All of the entries of A must be nonzero.

    \paragraph{Solution: }Take the matrix:
    \[ A=\begin{bmatrix} 2&1\\1&2\\2&1\\1&2\end{bmatrix};\quad
    A^* A=\begin{bmatrix} 2&1&2&1\\1&2&1&2 \end{bmatrix}
    \begin{bmatrix} 2&1\\1&2\\2&1\\1&2\end{bmatrix}=
    \begin{bmatrix} 10&8\\8&10 \end{bmatrix} 
.\]
    And compute $C_{A^* A}(z)=(z-10)^2-64=z^2-20z-36=(z-18)(z-2)$.
    So now we have our $\Sigma$:
     \[ \Sigma=\begin{bmatrix} 3\sqrt{2} &0\\0&\sqrt{2}  \end{bmatrix} .\]
    And singular values: $\sigma_1=3\sqrt{2},\sigma_2=\sqrt{2} $. Now compute eigenvectors for $\sigma_1^2,\sigma_2^2$ :
    \begin{align*}
        (A^* A-18I)v_1&= \begin{bmatrix} -8&8\\8&-8 \end{bmatrix}  \\
        v_1&=\frac{1}{\sqrt{2} } \begin{bmatrix} 1\\1 \end{bmatrix}  \\
        (A^* A-2I)v_2&= \begin{bmatrix} 8&8\\8&8 \end{bmatrix}  \\
        v_1&=\frac{1}{\sqrt{2} } \begin{bmatrix} 1\\-1 \end{bmatrix} 
    .\end{align*}
    So we have our $V$:
    \[ V=\frac{1}{\sqrt{2} }\begin{bmatrix} 1&1\\1&-1 \end{bmatrix} .\] 
    And we are free to compute $w_1,w_2$ from $v_1,v_2$:
    \begin{align*}
        w_1&= \frac{1}{\sigma_1}Av_1 \\
        &= \frac{1}{3\sqrt{2} } \\
        &=\frac{1}{3\sqrt{2} }\begin{bmatrix} 2&1\\1&2\\2&1\\1&2\end{bmatrix}\frac{1}{\sqrt{2} }\begin{bmatrix} 1\\1 \end{bmatrix} \\
        &= \frac{1}{6}\begin{bmatrix} 3\\3\\3\\3 \end{bmatrix}  \\
        &= \frac{1}{2}\begin{bmatrix} 1\\1\\1\\1 \end{bmatrix}  \\
        w_2&= \frac{1}{\sigma_2}Av_2 \\
        &= \frac{1}{\sqrt{2} } \\
        &=\frac{1}{\sqrt{2} }\begin{bmatrix} 2&1\\1&2\\2&1\\1&2\end{bmatrix}\frac{1}{\sqrt{2} }\begin{bmatrix} 1\\-1 \end{bmatrix} \\
        &= \frac{1}{2}\begin{bmatrix} 1\\-1\\1\\-1 \end{bmatrix}
    .\end{align*}
    Now we must extend $w_1,w_2$ to an orthonormal basis of $\mathbb{C}^{4}$. We could do this with Gram-Schmidt, or we could be brave and move negative signs around until all our inner products turn out to be zero. Opt for the second option, and find:
    \[
    w_3=\frac{1}{2}\begin{bmatrix} 1\\1\\-1\\-1 \end{bmatrix},\quad w_4=\frac{1}{2}\begin{bmatrix} -1\\1\\1\\-1 \end{bmatrix} 
    .\] 

    Then we take the transpose of all our $w_i$'s, and get our $W$:
    \[
    W^* =\frac{1}{2}
    \begin{bmatrix} 1&1&1&1\\
    1&-1&-1&1\\
    1&1&-1&-1\\
    -1&1&1&-1
    \end{bmatrix}
    .\] 
    And therefore we have our SVD:
     \[
    A=\frac{1}{2}
    \begin{bmatrix} 1&1&1&1\\
    1&-1&-1&1\\
    1&1&-1&-1\\
    -1&1&1&-1
\end{bmatrix}
\begin{bmatrix} 3\sqrt{2} &0\\0&\sqrt{2}  \end{bmatrix} 
\frac{1}{\sqrt{2} }\begin{bmatrix} 1&1\\1&-1 \end{bmatrix} 
=\begin{bmatrix} 2&1\\1&2\\2&1\\1&2\end{bmatrix}
    .\] 

    \newpage
\item For an $m \times  n$ matrix $A$, show that the set of nonzero eigenvalues for $A^*  A$ coincide with that of $AA^* $.
    \paragraph{Solution: }Let $0\neq \lambda\in \sigma(A^* A)$, with an associated eigenvector $v$.

    Then $A^* Av=\lambda v$. Applying $A$ on both sides, we have $A A^*  Av=A\lambda v=\lambda Av$, and so $Av$ is an eigenvector for $A A^* $ associated with $\lambda$.

    Now let $0~=\lambda\in \sigma(A A^* )$

    Then suppose $A A^*v=\lambda v$. Applying $A^* $ on both sides, we have $A ^*A  A^* v=A^* \lambda v=\lambda A^* v$, and so $A^*v$ is an eigenvector for $A ^*A$ associated with $\lambda$.

\item Suppose $A = W \Sigma V^* $ is a singular value decomposition for $A$. Show that the columns of $W$ are eigenvectors for $AA^* $.
    \paragraph{Solution: }Let $1\leq i \leq n$, and take:
    \[ W= \begin{bmatrix} w_1&\dots&w_n \end{bmatrix},\quad \quad 
    \Sigma=
    \begin{bmatrix} \sigma_{1}&&&0 \\
    &\ddots &&\vdots \\
    &&\sigma_r&0 \\
    0&\dots&0&0
    \end{bmatrix} .\] 
    Then begin the computation:
    \begin{align*}
        A A^* w_i&= W\Sigma V^* V \Sigma^* W^* w_i \\
                 &= W\Sigma^2W^*w_i &\text{$V$ unitary, $\Sigma$ real, symmetric} \\
                 &= W\Sigma^2\begin{bmatrix} w_1^* \\\vdots \\ w_n^*  \end{bmatrix}w_i  \\
                 &= W\Sigma^2\begin{bmatrix} w_1^*w_i \\\vdots \\ w_n^* w_i \end{bmatrix}\\
                 &= W\Sigma^2\begin{bmatrix} \langle w_i,w_1 \rangle \\\vdots \\ \langle w_i,w_n \rangle \end{bmatrix}\\
                 &= W\Sigma^2\begin{bmatrix} 0\\ \vdots\\ \langle w_i,w_i \rangle\\ \vdots \\ 0 \end{bmatrix}&\text{Since $w_i$ form an o.n.b.}\\
                 &= W\Sigma^2\begin{bmatrix} 0\\ \vdots\\ \|w_i\|^2\\ \vdots \\ 0 \end{bmatrix}
    .\end{align*}
    Now we split by cases. If $i>r$, then the  $i$-th column of $\Sigma$ will be exactly zero, and we will have $A A^* w_i=W0=0=0w_i$, and $w_i$ is an eigenvector associated with $0$.

    But if $i\leq r$, then the $i$-th column of $\Sigma^2$ will be of the form $\Sigma^2=\begin{bmatrix} 0& \dots& \sigma_i^2& \dots & 0 \end{bmatrix}^{T}$ 

    Then our equation becomes
    \begin{align*}
        A A^* w_i&=  W\sigma_i^2\begin{bmatrix} 0\\ \vdots\\ \|w_i\|^2\\ \vdots \\ 0 \end{bmatrix}\\
        &=  \sigma_i^2\begin{bmatrix} w_1&\dots&w_n \end{bmatrix}\begin{bmatrix} 0\\ \vdots\\ \|w_i\|^2\\ \vdots \\ 0 \end{bmatrix}\\
        &= (\sigma_i\|w_i\|)^2w_i 
    .\end{align*}
    And as we wanted to show, $w_i$ is an eigenvector for $A A^* $.
%\coffeestainA{0.9}{0.85}{-25}{5cm}{-4.0cm}
\end{enumerate}
\end{document}
